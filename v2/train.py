import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from keras.models import Sequential, load_model
from keras.layers import LSTM, Dense, Dropout, BatchNormalization
from keras.callbacks import EarlyStopping
import requests
from io import StringIO
import os
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer
import matplotlib.pyplot as plt
import datetime
from datetime import datetime, timedelta
from joblib import dump, load
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

def download_data(url):
    response = requests.get(url)
    data = pd.read_csv(StringIO(response.text))
    return data

def feature_engineering(data):
    # Rolling mean and standard deviation
    data['ROLLING_MEAN_5'] = data['LAST_PRICE'].rolling(window=5).mean()
    data['ROLLING_STD_5'] = data['LAST_PRICE'].rolling(window=5).std()

    # Lagged Features
    for i in range(1, 4):  # Adding lags for the past 3 time steps
        data[f'LAG_{i}'] = data['LAST_PRICE'].shift(i)

    # MACD
    data['EMA12'] = data['LAST_PRICE'].ewm(span=12).mean()
    data['EMA26'] = data['LAST_PRICE'].ewm(span=26).mean()
    data['MACD'] = data['EMA12'] - data['EMA26']

    # RSI
    delta = data['LAST_PRICE'].diff()
    gain = (delta.where(delta > 0, 0)).fillna(0)
    loss = (-delta.where(delta < 0, 0)).fillna(0)
    avg_gain = gain.rolling(window=14).mean()
    avg_loss = loss.rolling(window=14).mean()
    rs = avg_gain / avg_loss
    data['RSI'] = 100 - (100 / (1 + rs))

    # Drop NA values generated by rolling functions and technical indicators
    data.dropna(inplace=True)

    return data

def preprocess_data(data):
    # Feature Engineering
    data = feature_engineering(data)
    
    # Separate the target column
    target = data['LAST_PRICE']
    data = data.drop(['LAST_PRICE', 'TIME'], axis=1)  # Assuming 'TIME' is the timestamp and not used as a feature for training
    
    # Normalize the features
    scaler = MinMaxScaler(feature_range=(0, 1))
    data_normalized = scaler.fit_transform(data)
    
    # Normalize the target
    target_scaler = MinMaxScaler(feature_range=(0, 1))
    target_normalized = target_scaler.fit_transform(target.values.reshape(-1, 1))
    
    return data_normalized, target_normalized, scaler, target_scaler

def split_data(data_normalized, target_normalized):
    X_train, X_temp, y_train, y_temp = train_test_split(data_normalized, target_normalized, test_size=0.3, shuffle=False)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.67, shuffle=False)
    return X_train, y_train, X_val, y_val, X_test, y_test

def create_sequences(data, target, seq_length, steps_ahead=60):
    X, y = [], []
    for i in range(len(data) - seq_length - steps_ahead + 1):
        X.append(data[i:i+seq_length])
        y.append(target[i+seq_length:i+seq_length+steps_ahead])
    return np.array(X), np.array(y)

def train_model(model, X_train, y_train, X_val, y_val):
    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    history = model.fit(X_train, y_train, epochs=50, batch_size=60, validation_data=(X_val, y_val), shuffle=False, callbacks=[early_stop])
    model.save('bitcoin_lstm_model.h5')
    return history

def predict_next_60_minutes(model, last_60_minutes_data, target_scaler):
    current_sequence = last_60_minutes_data.reshape(1, last_60_minutes_data.shape[0], last_60_minutes_data.shape[1])
    
    # Predict the next 60 minutes in one go
    predictions = model.predict(current_sequence)
    
    # Convert predictions to their original scale
    predictions_original = target_scaler.inverse_transform(predictions[0])
    
    return predictions_original

def convert_to_local_time(timestamp):
    utc_time = datetime.utcfromtimestamp(timestamp)
    local_time = utc_time + timedelta(hours=3)  # Convert from UTC+0 to UTC+3
    return local_time.strftime('%H:%M:%S')

def visualize_predictions(timestamps, last_prediction):
    # Convert the last timestamp to local time and add 60 minutes
    last_time = convert_to_local_time(timestamps[-10])
    last_datetime = datetime.strptime(last_time, '%H:%M:%S') + timedelta(minutes=60)
    
    # Generate local times for the next 10 minutes
    local_times = [(last_datetime + timedelta(minutes=i)).strftime('%H:%M:%S') for i in range(1, 11)]
    
    # Plot the last prediction
    plt.figure(figsize=(10, 5))
    plt.plot(local_times, last_prediction[-10:], label='Predicted Prices', color='blue')
    plt.xlabel('Time (in H:M:S)')
    plt.ylabel('Bitcoin Price')
    plt.title('Bitcoin Price Predictions for the Next 10 Minutes')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

def create_model(input_shape, units=50):
    model = Sequential()
    model.add(LSTM(units, input_shape=input_shape, return_sequences=True))
    model.add(Dropout(0.2))
    model.add(BatchNormalization())
    model.add(LSTM(units, return_sequences=True))
    model.add(Dropout(0.2))
    model.add(BatchNormalization())
    model.add(Dense(60))
    model.compile(optimizer='adam', loss='mse')
    return model

def objective(params, X, y):
    units = int(params['units'])
    dropout = params['dropout']
    
    model = create_model((X.shape[1], X.shape[2]), units)
    
    history = model.fit(X, y, epochs=10, batch_size=60, validation_split=0.2, shuffle=False, verbose=0)
    
    val_loss = history.history['val_loss'][-1]
    return {'loss': val_loss, 'status': STATUS_OK}

def main():
    url = "https://bitcoin-data-collective-rzeraat.vercel.app/api/download_btc"
    data = download_data(url)
    data_normalized, target_normalized, scaler, target_scaler = preprocess_data(data)
    
    # Save the scalers
    dump(scaler, 'feature_scaler.pkl')
    dump(target_scaler, 'target_scaler.pkl')
    
    seq_length = 60
    X, y = create_sequences(data_normalized, target_normalized, seq_length)

    # Check if the model exists and load it, otherwise create a new one
    model_path = 'bitcoin_lstm_model.h5'
    if os.path.exists(model_path):
        print("Loading existing model...")
        model = load_model(model_path)
    else:
        print("Creating a new model...")
        # Hyperparameter tuning and model creation code here...
        space = {
            'units': hp.quniform('units', 60, 360, 60),
            'dropout': hp.uniform('dropout', 0.1, 0.5)
        }
        
        trials = Trials()
        best = fmin(lambda params: objective(params, X, y), space, algo=tpe.suggest, max_evals=10, trials=trials)
        
        best_units = int(best['units'])
        best_dropout = best['dropout']
        
        model = create_model((X.shape[1], X.shape[2]), best_units)

    # Train the model
    history = model.fit(X, y, epochs=50, batch_size=60, shuffle=False)
    
    # Save the model after training
    model.save(model_path)
    
    # Print the model summary
    model.summary()
    
    # Predict the next 60 minutes
    last_60_minutes_data = data_normalized[-60:]
    predictions_60 = predict_next_60_minutes(model, last_60_minutes_data, target_scaler)
    
    # Extract the last 10 timestamps from the original data
    last_10_timestamps = data['TIME'].values[-10:]

    # Extract the first 10 timestamps from the original data
    first_10_timestamps = data['TIME'].values[:10]
    
    print(predictions_60)
    
    # Visualize the last 10 minutes of predictions for the last sequence
    visualize_predictions(last_10_timestamps, predictions_60[-1][-10:])  
    visualize_predictions(first_10_timestamps, predictions_60[-1][:10])  

if __name__ == "__main__":
    main()

